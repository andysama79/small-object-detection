{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import einops\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(), # activation function - gaussian error linear unit\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, disp):\n",
    "        super().__init__()\n",
    "        self.disp = disp\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.disp, self.disp), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, 0., -inf, -inf, 0., -inf, -inf],\n",
       "        [-inf, 0., 0., -inf, 0., 0., -inf, 0., 0.],\n",
       "        [-inf, 0., 0., -inf, 0., 0., -inf, 0., 0.],\n",
       "        [0., -inf, -inf, 0., -inf, -inf, 0., -inf, -inf],\n",
       "        [-inf, 0., 0., -inf, 0., 0., -inf, 0., 0.],\n",
       "        [-inf, 0., 0., -inf, 0., 0., -inf, 0., 0.],\n",
       "        [0., -inf, -inf, 0., -inf, -inf, 0., -inf, -inf],\n",
       "        [-inf, 0., 0., -inf, 0., 0., -inf, 0., 0.],\n",
       "        [-inf, 0., 0., -inf, 0., 0., -inf, 0., 0.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_mask(window_size=3, displacement=2, upper_lower=False, left_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_dist(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "\n",
    "    dist = indices[None, :, :] - indices[:, None, :]\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9, 2])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
      "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
      "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
      "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
      "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
      "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
      "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
      "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
      "        [-2, -2, -2, -1, -1, -1,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "tmp = get_rel_dist(3)\n",
    "print(tmp.size())\n",
    "print(tmp[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, head_dim, shifted, window_size, rel_pos_emb):\n",
    "        super().__init__()\n",
    "        print(\"WindowAttention\")\n",
    "        inner_dim = head_dim * num_heads    #will be equal to number of channels sucj as (32*3 = 96 for the first layer)\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = head_dim ** -0.5   #scale factor for the attention mechanism (1/sqrt(d_k))\n",
    "        self.window_size = window_size  #for now, we will keep it as 7\n",
    "        self.shifted = shifted\n",
    "        self.rel_pos_emb = rel_pos_emb\n",
    "        if self.shifted:\n",
    "            disp = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-disp)\n",
    "            self.cyclic_shift_rev = CyclicShift(disp)\n",
    "        \n",
    "            self.top_bottom_mask = nn.Parameter(create_mask(window_size=window_size, displacement=disp, upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=disp, upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        # self.pos_emb = nn.Parameter(torch.randn(window_size**2, window_size**2))\n",
    "        # this is realtive position embedding for the window size\n",
    "        if self.rel_pos_emb:\n",
    "            self.rel_ind = get_rel_dist(window_size) + window_size - 1\n",
    "            self.pos_emb = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_emb = nn.Parameter(torch.randn(window_size**2, window_size**2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        batch, n_height, n_width, _, h = *x.shape, self.num_heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        num_window_h = n_height//self.window_size\n",
    "        num_window_w = n_width//self.window_size\n",
    "\n",
    "        q,k,v = map(lambda t: rearrange(t, 'batch (num_window_h w_h) (num_window_w w_w) (h d) -> batch h (num_window_h num_window_w) (w_h w_w) d', h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "\n",
    "        # this is a dot product similarity approch\n",
    "        # dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        # A better approch would be use the cosine similarity pointed out in the version 2 of the swin_t paper\n",
    "        #  better because : it an be beneficial for object detection tasks when there are many similar objects (e.g., different bird species) that need to be distinguished.\n",
    "        \n",
    "        self.tau = nn.Parameter(torch.tensor(0.02), requires_grad=True)\n",
    "        q = f.normalize(q, p=2, dim=-1)\n",
    "        k = f.normalize(k, p=2, dim=-1)\n",
    "        \n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau\n",
    "        \n",
    "        \n",
    "        # here the possition embedding is added to all the rows\n",
    "        # dots += self.pos_emb\n",
    "        if self.rel_pos_emb:\n",
    "            temp1 = self.rel_ind[:,:,0]\n",
    "            dots += self.pos_emb[self.rel_ind[:,:,0], self.rel_ind[:,:,1]]\n",
    "        else:\n",
    "            dots += self.pos_emb\n",
    "\n",
    "        if self.shifted:\n",
    "            #here mask are being added to the last rows be it bpttom or right\n",
    "            dots[:,:,-num_window_w:]+=self.top_bottom_mask\n",
    "            dots[:,:,num_window_w-1::num_window_w] += self.left_right_mask\n",
    "\n",
    "        attntion = dots.softmax(dim=-1)\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attntion, v)\n",
    "        out = rearrange(out, 'b h (num_window_h num_window_w) (w_h w_w) d -> b (num_window_h w_h) (num_window_w w_w) (h d)', h=h, w_h=self.window_size, w_w=self.window_size, num_window_h=num_window_h, num_window_w=num_window_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_shift_rev(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, fn, dim):\n",
    "        super().__init__()\n",
    "        print(\"attention block PreNorm\")\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.norm(self.fn(x, **kwargs))  #this is different from the paper1 of swin whre the preNorm is applied before the mlp and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        print(\"atten_block Residual\")\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin_Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, head_dim, mlp_dim, shifted,window_size, rel_pos_emb):\n",
    "        super().__init__()\n",
    "        print(\"Swin_Block\")\n",
    "        self.attention_block = Residual(PreNorm(WindowAttention(dim=dim, num_heads=num_heads, head_dim=head_dim, shifted=shifted, window_size=window_size, rel_pos_emb=rel_pos_emb), dim))\n",
    "        self.mlp_block = Residual(PreNorm(FeedForward(hidden_dim=mlp_dim, dim=dim), dim))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        print(\"returning from swin block\")\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_scaling_factor):\n",
    "        super().__init__()\n",
    "        self.patch_merge = nn.Conv2d(in_channels, out_channels, kernel_size=down_scaling_factor, stride=down_scaling_factor, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_merge(x).permute(0, 2, 3, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channel, hid_dim, layers, down_scaling_factor, num_heads, head_dim, window_size, rel_pos_emb):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'number of layers should be even'\n",
    "        self.patch_partition = PatchMerging_Conv(in_channels=in_channel, out_channels=hid_dim, down_scaling_factor=down_scaling_factor)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers//2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Swin_Block(dim=hid_dim, num_heads=num_heads, head_dim=head_dim, mlp_dim = hid_dim*4, shifted=False ,window_size=window_size, rel_pos_emb=rel_pos_emb),\n",
    "                Swin_Block(dim=hid_dim, num_heads=num_heads, head_dim=head_dim, mlp_dim = hid_dim*4, shifted=True ,window_size=window_size, rel_pos_emb=rel_pos_emb),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        for regular, shifted in self.layers:\n",
    "            x = regular(x)\n",
    "            x = shifted(x)\n",
    "        print(\"returning from stage module\")\n",
    "        return x.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hid_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7, down_scaling_fact=(4,2,2,2), rel_pos_emb = True):\n",
    "      super().__init__()\n",
    "      self.stage1 = StageModule(in_channel = channels, hid_dim=hid_dim, layers=layers[0], down_scaling_factor=down_scaling_fact[0], num_heads=heads[0], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "      self.stage2 = StageModule(in_channel = hid_dim, hid_dim=hid_dim*2, layers=layers[1], down_scaling_factor=down_scaling_fact[1], num_heads=heads[1], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "      self.stage3 = StageModule(in_channel = hid_dim*2, hid_dim=hid_dim*4, layers=layers[2], down_scaling_factor=down_scaling_fact[2], num_heads=heads[2], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "      self.stage4 = StageModule(in_channel = hid_dim*4, hid_dim=hid_dim*8, layers=layers[3], down_scaling_factor=down_scaling_fact[3], num_heads=heads[3], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "\n",
    "      self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hid_dim*8),\n",
    "            nn.Linear(hid_dim*8, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        x = self.stage1(image)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = x.mean(dim=[2,3])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_t(hid_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hid_dim=hid_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten_block Residual\n",
      "Swin_Block\n",
      "WindowAttention\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "attention block PreNorm\n",
      "atten_block Residual\n",
      "dummy called\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from stage module\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from stage module\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from stage module\n",
      "returning from swin block\n",
      "returning from swin block\n",
      "returning from stage module\n",
      "tensor([[-0.5030,  1.1040,  0.1388]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = swin_t(hid_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), channels=3, num_classes=3, head_dim=32, window_size=7, down_scaling_fact=(4,2,2,2), rel_pos_emb = True)\n",
    "\n",
    "dummy_x = torch.randn(1, 3, 224, 224)\n",
    "print(\"dummy called\")\n",
    "logits = net(dummy_x)  # (1, 3)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UpMerging(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(UpMerging, self).__init__()\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)  # PixelShuffle with stride 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply PixelShuffle to up-sample the feature maps\n",
    "        x = self.pixel_shuffle(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# Define input tensor shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(1, 64, 7, 7)  # Example input tensor shape\n",
    "\n",
    "# Define up-merging module\n",
    "um = UpMerging(in_channels=64)\n",
    "\n",
    "# Perform up-merging\n",
    "output_tensor = um(input_tensor)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor.shape)  # Output: torch.Size([1, 16, 14, 14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
