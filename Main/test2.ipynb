{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbone import Backbone\n",
    "from neck import Neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_model = Backbone(hid_dim=96, layers=(2, 2, 2, 2), heads=(3, 6, 12, 24))#.to('cuda')\n",
    "# back_out, feature_maps = back_model(torch.randn(1, 3, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'back_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mback_out\u001b[49m\u001b[38;5;241m.\u001b[39mshape, [f\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feature_maps]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'back_out' is not defined"
     ]
    }
   ],
   "source": [
    "back_out.shape, [f.shape for f in feature_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neck_model = Neck(hid_dim=96, layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), channels=768)#.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'back_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m neck_out \u001b[38;5;241m=\u001b[39m neck_model(\u001b[43mback_out\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), feature_maps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'back_out' is not defined"
     ]
    }
   ],
   "source": [
    "neck_out = neck_model(back_out.permute(0, 3, 1, 2), feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neck_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mneck_out\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neck_out' is not defined"
     ]
    }
   ],
   "source": [
    "neck_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from head import Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = Head(in_channels=96, num_classes=1)#.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (backbone): Backbone(\n",
       "    (model): SwinTransformer(\n",
       "      (stage1): StageModule(\n",
       "        (patch_partition): PatchMerging(\n",
       "          (patch_merge): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "                    (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "                    (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage2): StageModule(\n",
       "        (patch_partition): PatchMerging(\n",
       "          (patch_merge): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "                    (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "                    (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage3): StageModule(\n",
       "        (patch_partition): PatchMerging(\n",
       "          (patch_merge): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                    (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                    (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage4): StageModule(\n",
       "        (patch_partition): PatchMerging(\n",
       "          (patch_merge): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                    (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                    (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Neck(\n",
       "    (model): SwinTransformerNeck(\n",
       "      (stage1): StageModule(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=768, out_features=288, bias=False)\n",
       "                    (to_out): Linear(in_features=96, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=384, out_features=768, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=768, out_features=288, bias=False)\n",
       "                    (to_out): Linear(in_features=96, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=384, out_features=768, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (up_sample): UpMerging(\n",
       "          (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
       "        )\n",
       "      )\n",
       "      (stage2): StageModule(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=384, out_features=576, bias=False)\n",
       "                    (to_out): Linear(in_features=192, out_features=384, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=384, out_features=768, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=384, out_features=576, bias=False)\n",
       "                    (to_out): Linear(in_features=192, out_features=384, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=384, out_features=768, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (up_sample): UpMerging(\n",
       "          (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
       "        )\n",
       "      )\n",
       "      (stage3): StageModule(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=192, out_features=1152, bias=False)\n",
       "                    (to_out): Linear(in_features=384, out_features=192, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=192, out_features=1536, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=1536, out_features=192, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=192, out_features=1152, bias=False)\n",
       "                    (to_out): Linear(in_features=384, out_features=192, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=192, out_features=1536, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=1536, out_features=192, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (up_sample): UpMerging(\n",
       "          (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
       "        )\n",
       "      )\n",
       "      (stage4): StageModule(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (to_qkv): Linear(in_features=96, out_features=2304, bias=False)\n",
       "                    (to_out): Linear(in_features=768, out_features=96, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=96, out_features=3072, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=3072, out_features=96, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (attention_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (cyclic_shift): CyclicShift()\n",
       "                    (cyclic_shift_rev): CyclicShift()\n",
       "                    (to_qkv): Linear(in_features=96, out_features=2304, bias=False)\n",
       "                    (to_out): Linear(in_features=768, out_features=96, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (mlp_block): Residual(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): FeedForward(\n",
       "                    (network): Sequential(\n",
       "                      (0): Linear(in_features=96, out_features=3072, bias=True)\n",
       "                      (1): GELU(approximate='none')\n",
       "                      (2): Linear(in_features=3072, out_features=96, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (up_sample): UpMerging(\n",
       "          (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Head(\n",
       "    (conv_heatmap): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_width): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_height): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_offset): Conv2d(96, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = Model(back_model, neck_model, head_model)\n",
    "mod#.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.randn(1, 3, 1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 32, 32, 768]), Feature map shape: torch.Size([1, 32, 32, 768])\n",
      "here\n",
      "here\n",
      "here\n",
      "features:  torch.Size([1, 256, 256, 96])\n",
      "torch.Size([1, 96, 256, 256])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "mod_out = mod(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(back_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(neck_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.randn(1, 32, 32, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schecter/miniconda3/envs/pikachu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from swin_fpn_neck import SwinFPNNeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SwinFPNNeck(channels=768, hid_dim=96, layers=(2, 2, 2, 2), heads=(3, 6, 12, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = ([torch.randn(1, 32, 32, 768), torch.randn(1, 64, 64, 384), torch.randn(1, 128, 128, 192), torch.randn(1, 256, 256, 96)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256, 96])\n",
      "torch.Size([1, 128, 128, 192])\n",
      "torch.Size([1, 64, 64, 384])\n",
      "torch.Size([1, 32, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(feature_maps[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([1, 384, 32, 32])\n",
      "X shape:  torch.Size([1, 384, 32, 32])\n",
      "X1 shape: torch.Size([1, 64, 64, 384]) X2 shape: torch.Size([1, 64, 64, 384])\n",
      "X shape:  torch.Size([1, 192, 64, 64])\n",
      "X shape:  torch.Size([1, 192, 64, 64])\n",
      "X shape:  torch.Size([1, 96, 128, 128])\n",
      "X shape:  torch.Size([1, 96, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 96])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod(temp, feature_maps).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(1, 32, 32, 768)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pikachu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
