{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "from einops import rearrange\n",
    "import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Libraries \n",
    "from backbone import Backbone\n",
    "from neck_bak import swin_t_neck\n",
    "from neck import Neck\n",
    "from head import Head\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"your_project_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating ground truth\n",
    "\n",
    "def calculate_offset(image_size, bbox):\n",
    "    x_center = (bbox[0] + bbox[2]) / 2\n",
    "    y_center = (bbox[1] + bbox[3]) / 2\n",
    "    offset_x = x_center / image_size[1]\n",
    "    offset_y = y_center / image_size[0]\n",
    "    return offset_x, offset_y\n",
    "\n",
    "def calculate_width_height(bbox):\n",
    "    width = bbox[2]\n",
    "    height = bbox[3]\n",
    "    return width, height\n",
    "\n",
    "def generate_ground_truth(image_size, bboxs):\n",
    "    heatmap = np.zeros(image_size)\n",
    "    widthmap = np.zeros(image_size)\n",
    "    heightmap = np.zeros(image_size)\n",
    "    offsetmap = np.zeros((2,image_size[0], image_size[1]))\n",
    "    for bbox in bboxs:\n",
    "        x_center = int((bbox[0] + bbox[2]) / 2)\n",
    "        y_center = int((bbox[1] + bbox[3]) / 2)\n",
    "        heatmap[x_center, y_center] = 1 \n",
    "        width, height = calculate_width_height(bbox) # Set the center of the bounding box to 1\n",
    "        widthmap[x_center, y_center] = width\n",
    "        heightmap[x_center, y_center] =  height\n",
    "        offset = calculate_offset(image_size, bbox)\n",
    "        offsetmap[0,x_center, y_center] = offset[0]\n",
    "        offsetmap[1,x_center, y_center] = offset[1]\n",
    "        \n",
    "        print(f\"X_center: {x_center}, Y_center: {y_center}, Width: {width}, Height: {height}, Offset: {offset}\")\n",
    "    return heatmap, widthmap, heightmap, offsetmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_predictions(preds: list[torch.tensor], tgt_size: np.ndarray, intensity_thresh: float=0.8):\n",
    "    heat, w, h, o = preds\n",
    "\n",
    "    heat = heat.detach().numpy()\n",
    "    heat = np.where(heat > intensity_thresh, heat, 0)\n",
    "    heat = torch.tensor(heat)\n",
    "    heat = f.interpolate(heat.float(), size=tgt_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "    w = f.interpolate(w.float(), size=tgt_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "    h = f.interpolate(h.float(), size=tgt_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "    o = f.interpolate(o.float(), size=tgt_size, mode='bilinear', align_corners=False)\n",
    "    # o[1] = f.interpolate(o[1].float(), size=tgt_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "    heat = einops.rearrange(heat, 'b c h w -> h (w c b)')\n",
    "    heat = heat.detach().numpy()\n",
    "    w = einops.rearrange(w, 'b c h w -> h (w c b)')\n",
    "    w = w.detach().numpy()\n",
    "    h = einops.rearrange(h, 'b c h w -> h (w c b)')\n",
    "    h = h.detach().numpy()\n",
    "    o = einops.rearrange(o, 'b c h w -> (b c) h w')\n",
    "    o = o.detach().numpy()\n",
    "\n",
    "\n",
    "    heat = np.where(heat==0, heat, 0)\n",
    "    w = np.where(heat==0, w, 0)\n",
    "    h = np.where(heat==0, h, 0)\n",
    "    o = np.where(heat==0, o, 0)\n",
    "    # o[1] = np.where(heat==0, o[1], 0\n",
    "\n",
    "    return heat, w, h, o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(preds, truth):\n",
    "    p_heat, p_w, p_h, p_o = preds\n",
    "    g_heat, g_w, g_h, g_o = truth\n",
    "    heatmap_loss = f.binary_cross_entropy_with_logits(f.sigmoid(p_heat), f.sigmoid(g_heat))\n",
    "    width_loss = f.l1_loss(p_w, g_w)\n",
    "    height_loss = f.l1_loss(p_h, g_h)\n",
    "    offset_loss = f.l1_loss(p_o, g_o)\n",
    "    return heatmap_loss + width_loss + height_loss + offset_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = Backbone(hid_dim=96, layers=[2, 2, 2, 2], heads=[3, 6, 12, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neck_t = Neck(hid_dim=96, layers=[2,2,2,2], heads=[24, 12, 6, 3], channels=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Head(in_channels=96, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(backbone, neck_t, head)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "IMG_SIZE = (2160,3840)\n",
    "\n",
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is how input is defined\n",
    "# img = cv2.cvtColor(cv2.imread(\"../assets/sample_run.jpeg\"), cv2.COLOR_BGR2RGB)\n",
    "# img = cv2.resize(img, (1600, 896))\n",
    "# data = torch.tensor(img).unsqueeze(0).float()\n",
    "# data = data.permute(0, 3, 1, 2)\n",
    "# print(data.shape)\n",
    "# plt.imshow(img)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# #These are the bounding box of all the small objects.\n",
    "# bbox_values = torch.tensor([[2035,1003,9,17], [795,1169,9,17], [2715,1524,9,17], [263,209,9,17], [931,844,9,17], [1621,1398,9,17]]).float()\n",
    "# #Other bbox values are [795,1169,9,17], [2715,1524,9,17], [263,209,9,17], [931,844,9,17], [1621,1398,9,17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm.tqdm(total=len(train_data), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        preds = model(data)\n",
    "        preds = upscale_predictions(preds, tgt_size=IMG_SIZE, intensity_thresh=THRESHOLD)\n",
    "        targets = generate_ground_truth(IMG_SIZE, bboxes)\n",
    "        loss = loss(preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        # Log loss to WandB\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pikachu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
