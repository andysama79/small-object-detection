{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as f\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import einops\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2016, 2016])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"../assets/annotated_image.jpg\", cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (2016, 2016))\n",
    "data = torch.tensor(img).unsqueeze(0).float()\n",
    "data = data.permute(0, 3, 1, 2)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swin_T as swin\n",
    "from importlib import reload\n",
    "# reload(swin)\n",
    "from swin_T import swin_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandanyadav/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "backbone = swin_t()\n",
    "o1 = backbone(data)\n",
    "o1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(), # activation function - gaussian error linear unit\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, disp):\n",
    "        super().__init__()\n",
    "        self.disp = disp\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.disp, self.disp), dims=(1, 2))\n",
    "\n",
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_rel_dist(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "\n",
    "    dist = indices[None, :, :] - indices[:, None, :]\n",
    "    return dist\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, head_dim, shifted, window_size, rel_pos_emb):\n",
    "        super().__init__()\n",
    "        # print(\"WindowAttention\")\n",
    "        inner_dim = head_dim * num_heads    #will be equal to number of channels sucj as (32*3 = 96 for the first layer)\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = head_dim ** -0.5   #scale factor for the attention mechanism (1/sqrt(d_k))\n",
    "        self.window_size = window_size  #for now, we will keep it as 7\n",
    "        self.shifted = shifted\n",
    "        self.rel_pos_emb = rel_pos_emb\n",
    "        if self.shifted:\n",
    "            disp = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-disp)\n",
    "            self.cyclic_shift_rev = CyclicShift(disp)\n",
    "        \n",
    "            self.top_bottom_mask = nn.Parameter(create_mask(window_size=window_size, displacement=disp, upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=disp, upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        # self.pos_emb = nn.Parameter(torch.randn(window_size**2, window_size**2))\n",
    "        # this is realtive position embedding for the window size\n",
    "        if self.rel_pos_emb:\n",
    "            self.rel_ind = get_rel_dist(window_size) + window_size - 1\n",
    "            self.pos_emb = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_emb = nn.Parameter(torch.randn(window_size**2, window_size**2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        batch, n_height, n_width, _, h = *x.shape, self.num_heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        num_window_h = n_height//self.window_size\n",
    "        num_window_w = n_width//self.window_size\n",
    "\n",
    "        q,k,v = map(lambda t: rearrange(t, 'batch (num_window_h w_h) (num_window_w w_w) (h d) -> batch h (num_window_h num_window_w) (w_h w_w) d', h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "\n",
    "        # this is a dot product similarity approch\n",
    "        # dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        # A better approch would be use the cosine similarity pointed out in the version 2 of the swin_t paper\n",
    "        #  better because : it an be beneficial for object detection tasks when there are many similar objects (e.g., different bird species) that need to be distinguished.\n",
    "        \n",
    "        self.tau = nn.Parameter(torch.tensor(0.02), requires_grad=True)\n",
    "        q = f.normalize(q, p=2, dim=-1)\n",
    "        k = f.normalize(k, p=2, dim=-1)\n",
    "        \n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau\n",
    "        \n",
    "        \n",
    "        # here the possition embedding is added to all the rows\n",
    "        # dots += self.pos_emb\n",
    "        if self.rel_pos_emb:\n",
    "            temp1 = self.rel_ind[:,:,0]\n",
    "            dots += self.pos_emb[self.rel_ind[:,:,0], self.rel_ind[:,:,1]]\n",
    "        else:\n",
    "            dots += self.pos_emb\n",
    "\n",
    "        if self.shifted:\n",
    "            #here mask are being added to the last rows be it bpttom or right\n",
    "            dots[:,:,-num_window_w:]+=self.top_bottom_mask\n",
    "            dots[:,:,num_window_w-1::num_window_w] += self.left_right_mask\n",
    "\n",
    "        attntion = dots.softmax(dim=-1)\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attntion, v)\n",
    "        out = rearrange(out, 'b h (num_window_h num_window_w) (w_h w_w) d -> b (num_window_h w_h) (num_window_w w_w) (h d)', h=h, w_h=self.window_size, w_w=self.window_size, num_window_h=num_window_h, num_window_w=num_window_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_shift_rev(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, fn, dim):\n",
    "        super().__init__()\n",
    "        # print(\"attention block PreNorm\")\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.norm(self.fn(x, **kwargs))  #this is different from the paper1 of swin whre the preNorm is applied before the mlp and attention\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        # print(\"atten_block Residual\")\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class Swin_Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, head_dim, mlp_dim, shifted,window_size, rel_pos_emb):\n",
    "        super().__init__()\n",
    "        # print(\"Swin_Block\")\n",
    "        self.attention_block = Residual(PreNorm(WindowAttention(dim=dim, num_heads=num_heads, head_dim=head_dim, shifted=shifted, window_size=window_size, rel_pos_emb=rel_pos_emb), dim))\n",
    "        self.mlp_block = Residual(PreNorm(FeedForward(hidden_dim=mlp_dim, dim=dim), dim))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        # print(\"returning from swin block\")\n",
    "        return x\n",
    "        \n",
    "class PatchMerging_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_scaling_factor):\n",
    "        super().__init__()\n",
    "        self.patch_merge = nn.Conv2d(in_channels, out_channels, kernel_size=down_scaling_factor, stride=down_scaling_factor, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_merge(x).permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channel, hid_dim, layers, up_scaling_factor, num_heads, head_dim, window_size, rel_pos_emb):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'number of layers should be even'\n",
    "        # self.patch_partition = PatchMerging_Conv(in_channels=in_channel, out_channels=hid_dim, down_scaling_factor=up_scaling_factor)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers//2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Swin_Block(dim=in_channel, num_heads=num_heads, head_dim=head_dim, mlp_dim = hid_dim*4, shifted=False ,window_size=window_size, rel_pos_emb=rel_pos_emb),\n",
    "                Swin_Block(dim=in_channel, num_heads=num_heads, head_dim=head_dim, mlp_dim = hid_dim*4, shifted=True ,window_size=window_size, rel_pos_emb=rel_pos_emb),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.patch_partition(x)\n",
    "        for regular, shifted in self.layers:\n",
    "            x = regular(x)\n",
    "            print(x.shape)\n",
    "            x = shifted(x)\n",
    "            print(x.shape)\n",
    "        # print(\"returning from stage module\")\n",
    "        return x.permute(0,3,1,2)\n",
    "    \n",
    "class UpMerging(nn.Module):\n",
    "    def __init__(self, in_channels, stride=2, up_scaling_factor=2):\n",
    "        super(UpMerging, self).__init__()\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)  # PixelShuffle with stride 2\n",
    "        self.linear = nn.Linear(in_channels//(stride)**2, (in_channels//(stride)**2)*up_scaling_factor)\n",
    "        # self.linear = nn.Linear(in_channels//(stride)**2, (in_channels//4)*upscaling_factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply PixelShuffle to up-sample the feature maps\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.linear(x)\n",
    "        # Apply linear layer to increase the number of channels\n",
    "        # x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hid_dim, layers, heads, channels, num_classes=1, head_dim=32, window_size=2, up_scaling_fact=(4,2,2,2), rel_pos_emb = True):\n",
    "      super().__init__()\n",
    "      self.stage1 = StageModule(in_channel = channels, hid_dim=hid_dim, layers=layers[0], up_scaling_factor=up_scaling_fact[0], num_heads=heads[0], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "      exit()\n",
    "      self.stage2 = StageModule(in_channel = hid_dim, hid_dim=hid_dim*2, layers=layers[1], up_scaling_factor=up_scaling_fact[1], num_heads=heads[1], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "      self.stage3 = StageModule(in_channel = hid_dim*2, hid_dim=hid_dim*4, layers=layers[2], up_scaling_factor=up_scaling_fact[2], num_heads=heads[2], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "      self.stage4 = StageModule(in_channel = hid_dim*4, hid_dim=hid_dim*8, layers=layers[3], up_scaling_factor=up_scaling_fact[3], num_heads=heads[3], head_dim=head_dim, window_size=window_size, rel_pos_emb=rel_pos_emb)\n",
    "\n",
    "    #   self.mlp_head = nn.Sequential(\n",
    "    #         nn.LayerNorm(hid_dim*8),\n",
    "    #         nn.Linear(hid_dim*8, num_classes)\n",
    "    #     )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        x = self.stage1(image)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        # x = x.mean(dim=[2,3])\n",
    "        # return self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "def swin_t_neck(hid_dim=96, layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hid_dim=hid_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neck = swin_t_neck(channels = 768)\n",
    "o2 = neck(o1.permute(0, 3, 2, 1))\n",
    "o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
